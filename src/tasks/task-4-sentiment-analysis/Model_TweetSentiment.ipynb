{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model_TweetSentiment",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4n5jkKWTNIUT",
        "outputId": "2552a03d-418e-4de7-cf09-3af019ba45f9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mOzMW86Qk94"
      },
      "source": [
        "datapath = '/content/drive/MyDrive/Omdena-India-Socio-Economy/'\n",
        "#file1 = datapath + 'tweet_with_transformer_labels.csv'\n",
        "file1 = datapath + 'Tweet_With_TextBlob_labels.csv'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDhAz1azTRM2"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a8ULLa1Tkcc"
      },
      "source": [
        "data = pd.read_csv(file1)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gbZRgl4UFYk",
        "outputId": "78b411c5-324d-4c47-9c15-af8e42dfea81"
      },
      "source": [
        "len(data['text'])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109533"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_74MPk6lcwv"
      },
      "source": [
        "a = list(data['text'])\n",
        "label = np.array((data['label']))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gk9OXy5hmNUM"
      },
      "source": [
        "# Convert list to collection of strings\n",
        "alltext = ' '.join([c for c in a]) \n",
        "# Convert \n",
        "words = alltext.split(' ')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGShPgJ8unOM"
      },
      "source": [
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mivgeRUgjMY"
      },
      "source": [
        "text_to_int = []\n",
        "for i, tweet in enumerate(a):\n",
        "  text_to_int.append([vocab_to_int[word] for word in tweet.split(' ')])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8OK-5Pn5jsFS",
        "outputId": "2e84ee99-0e24-4623-f205-841995e7cf6c"
      },
      "source": [
        "a[10]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'support fight corona viruse jai hind'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KrCX3UEjzPm",
        "outputId": "ed3224d5-bc7d-454c-d398-9e9f0ab50a5b"
      },
      "source": [
        "text_to_int[10]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[110, 36, 2, 4803, 284, 450]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAbOn8wD10O0",
        "outputId": "8c8363a6-3908-42aa-c395-579af7766985"
      },
      "source": [
        "vocab_to_int['fight']"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "36"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWOaCJ52AYFT",
        "outputId": "1908e3ae-9e92-424d-f698-1997d58b7b7e"
      },
      "source": [
        "# stats about vocabulary\n",
        "print('Unique words: ', len((vocab_to_int)))  \n",
        "print()\n",
        "\n",
        "# print tokens in first review\n",
        "print('Tokenized tweet: \\n', text_to_int[:10])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique words:  60398\n",
            "\n",
            "Tokenized tweet: \n",
            " [[4900, 291, 441, 2, 48, 10105, 123], [1107, 744, 18, 685, 2837, 942, 7478, 30245], [2594, 3896, 31, 79, 1242, 60, 914, 847, 61, 558, 43, 67, 133, 3897, 1827, 22546, 578, 1767, 848, 59, 1419, 600, 1351, 470, 762, 2, 75], [9, 603, 3820, 13, 688], [111, 544, 161, 56, 70, 1780, 31, 133, 13288, 8364, 100, 2119, 1529, 2119, 100, 839, 754, 87, 5, 2, 105, 1562, 189, 252, 284, 450], [16264, 41, 5972, 60, 5097, 1518, 43, 489, 349, 228, 4187, 403, 133, 407, 3897, 578, 1767, 848, 59, 13, 156, 600, 1266, 470, 2031, 2, 75, 3771, 1039, 165, 96], [2594, 3896, 31, 79, 1242, 60, 914, 847, 61, 558, 43, 67, 133, 3897, 1827, 22546, 578, 1767, 848, 59, 1419, 600, 1351, 470, 762, 2, 75], [41, 5548, 22, 60, 1735, 1408, 133, 225, 4, 282, 403, 133, 1827, 407, 225, 578, 1767, 848, 59, 13, 156, 332, 22, 257, 341, 470, 393, 2, 75], [16264, 22, 60, 1735, 1408, 133, 225, 4, 282, 403, 133, 1827, 407, 225, 578, 1767, 848, 59, 13, 156, 332, 22, 257, 341, 470, 393, 2, 75], [36, 2, 148, 383, 40, 4064, 10714, 11411, 19, 9600, 30246, 44, 3145, 605, 16265, 13289, 30247, 40, 11411, 584, 2507, 31, 116, 3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCSO8fN3eXX_",
        "outputId": "1a795e5f-01e9-4b41-e1ee-7dabeb87f271"
      },
      "source": [
        "# Check that there are no zero length tweets. - these should have been removed during labeling but good to check\n",
        "zero_idx = [ii for ii, tweet in enumerate(text_to_int) if len(tweet) == 0]\n",
        "len(zero_idx)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crcrlZOAyDdP"
      },
      "source": [
        "Padding **sequences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyL-bDBiyf7U",
        "outputId": "c8bca308-d8c1-4c2a-865b-9a0c974640bd"
      },
      "source": [
        "# Count the length of tweets\n",
        "tweet_lens = Counter([len(x) for i, x in enumerate(text_to_int)])\n",
        "print(\"Maximum review length: {}\".format(max(tweet_lens)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum review length: 51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1S6F5Wm1Jrj"
      },
      "source": [
        "seq_length = max(tweet_lens)\n",
        "\n",
        "def pad_features(text_to_int, seq_length):\n",
        "\n",
        "  features = np.zeros((len(text_to_int), seq_length), dtype=int)\n",
        "  for i, row in enumerate(text_to_int):\n",
        "    features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "\n",
        "  return features"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlPCvh9q4al9"
      },
      "source": [
        "features = pad_features(text_to_int, seq_length)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzbWG41U6UlA"
      },
      "source": [
        "## test statements - do not change - ##\n",
        "assert len(features)==len(text_to_int), \"Features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\""
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-20q5LT9PYJ"
      },
      "source": [
        "## **Training**, **Validation**, **Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvZguQ7i9Qah",
        "outputId": "d54d7a88-9a1f-4ea5-c1b3-805c3e2e6721"
      },
      "source": [
        "split_frac = 0.8\n",
        "split_idx = int(len(features)*split_frac)\n",
        "split_idx"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87626"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-V_z6k7Br7l"
      },
      "source": [
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = label[:split_idx], label[split_idx:]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_e87pl4vwnq"
      },
      "source": [
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRFbcvn4xJ7g",
        "outputId": "796340f8-5b93-4466-dce7-b919372047b6"
      },
      "source": [
        "## print out the shapes of your resultant feature data\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(87626, 51) \n",
            "Validation set: \t(10953, 51) \n",
            "Test set: \t\t(10954, 51)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7XBHS-9xWjK"
      },
      "source": [
        "**DataLoaders and Batching**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ymaOnPQl_qD"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uhb478K7l-U_",
        "outputId": "05dc8e4c-41f7-4ca3-fc49-0bd0568031a7"
      },
      "source": [
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training on GPU.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnLOUXdI6QsK"
      },
      "source": [
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 128\n",
        "\n",
        "# make sure the SHUFFLE your training data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnM-Xkj8RAXU",
        "outputId": "5b6cee41-0803-412e-95fb-a0105e404fb1"
      },
      "source": [
        "len(train_x)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87626"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3H2UqmjFNP3",
        "outputId": "5590efb2-4ede-42f9-c336-ba62855d1f46"
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([128, 51])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,  1638,   575,   579],\n",
            "        [    0,     0,     0,  ...,  1115, 47277,   382],\n",
            "        [    0,     0,     0,  ...,   595,  5126,     6],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,    61,  2897,    73],\n",
            "        [    0,     0,     0,  ...,   152,   934,   809],\n",
            "        [    0,     0,     0,  ...,   352,     2,    10]])\n",
            "\n",
            "Sample label size:  torch.Size([128])\n",
            "Sample label: \n",
            " tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
            "        1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1,\n",
            "        0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
            "        0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
            "        0, 0, 0, 1, 1, 1, 1, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2TGFdCU28WF"
      },
      "source": [
        "# Sentiment Network with PyTorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh2pM6vR29NA"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SentimentRNN(nn.Module):\n",
        "    \"\"\"\n",
        "    The RNN model that will be used to perform Sentiment analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super(SentimentRNN, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and lstm_out\n",
        "        x = x.long()\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "    \n",
        "        # stack up lstm outputs\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFuAvSxr77TN",
        "outputId": "dcaaa223-e043-4e59-860d-e42f3770bb82"
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "epochs = 4\n",
        "\n",
        "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SentimentRNN(\n",
            "  (embedding): Embedding(60399, 400)\n",
            "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyIpc8pG9qHx"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvowpsWV9rXz"
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.01\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Av_APBKB_ZiF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1b23371-73e1-4910-a74f-e299caa109da"
      },
      "source": [
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "    net.cuda()\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if(train_on_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.346826... Val Loss: 0.393772\n",
            "Epoch: 1/4... Step: 200... Loss: 0.361022... Val Loss: 0.299911\n",
            "Epoch: 1/4... Step: 300... Loss: 0.170600... Val Loss: 0.285974\n",
            "Epoch: 1/4... Step: 400... Loss: 0.229966... Val Loss: 0.226250\n",
            "Epoch: 1/4... Step: 500... Loss: 0.263847... Val Loss: 0.195669\n",
            "Epoch: 1/4... Step: 600... Loss: 0.282685... Val Loss: 0.203124\n",
            "Epoch: 2/4... Step: 700... Loss: 0.074071... Val Loss: 0.178758\n",
            "Epoch: 2/4... Step: 800... Loss: 0.099148... Val Loss: 0.185602\n",
            "Epoch: 2/4... Step: 900... Loss: 0.071918... Val Loss: 0.159428\n",
            "Epoch: 2/4... Step: 1000... Loss: 0.072795... Val Loss: 0.161333\n",
            "Epoch: 2/4... Step: 1100... Loss: 0.072440... Val Loss: 0.146273\n",
            "Epoch: 2/4... Step: 1200... Loss: 0.075999... Val Loss: 0.148477\n",
            "Epoch: 2/4... Step: 1300... Loss: 0.094611... Val Loss: 0.141851\n",
            "Epoch: 3/4... Step: 1400... Loss: 0.089874... Val Loss: 0.150281\n",
            "Epoch: 3/4... Step: 1500... Loss: 0.130800... Val Loss: 0.158352\n",
            "Epoch: 3/4... Step: 1600... Loss: 0.051091... Val Loss: 0.186252\n",
            "Epoch: 3/4... Step: 1700... Loss: 0.076079... Val Loss: 0.147534\n",
            "Epoch: 3/4... Step: 1800... Loss: 0.091958... Val Loss: 0.150679\n",
            "Epoch: 3/4... Step: 1900... Loss: 0.057864... Val Loss: 0.153110\n",
            "Epoch: 3/4... Step: 2000... Loss: 0.028194... Val Loss: 0.132675\n",
            "Epoch: 4/4... Step: 2100... Loss: 0.016945... Val Loss: 0.177217\n",
            "Epoch: 4/4... Step: 2200... Loss: 0.093396... Val Loss: 0.139707\n",
            "Epoch: 4/4... Step: 2300... Loss: 0.046469... Val Loss: 0.154231\n",
            "Epoch: 4/4... Step: 2400... Loss: 0.047431... Val Loss: 0.161583\n",
            "Epoch: 4/4... Step: 2500... Loss: 0.016009... Val Loss: 0.141380\n",
            "Epoch: 4/4... Step: 2600... Loss: 0.060352... Val Loss: 0.149337\n",
            "Epoch: 4/4... Step: 2700... Loss: 0.138631... Val Loss: 0.147537\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFp4qek9SVUA"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "We9RRf-ESWFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa9d9223-4b19-45aa-fa06-fdb8223d0024"
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.145\n",
            "Test accuracy: 0.944\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}