# -*- coding: utf-8 -*-
"""SentimentInference.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qxhOEdd9Y_250u6ZeY6ZWXN4luQhm5kU
"""

import os
import pickle
import re
import shutil
import string
import datetime
import time
from string import punctuation

import nltk
import numpy as np
import pandas as pd
import csv
import streamlit as st
import torch
import torch.nn as nn
from emoji import demojize
from lxml import html
from collections import Counter
import plotly.express as px
import plotly.graph_objects as go

from calendar import monthrange
from datetime import datetime

nltk.download('stopwords')
from nltk.corpus import stopwords

from apps.Sentiment_Analysis.download import load

stop = set(stopwords.words('english'))



import apps.Sentiment_Analysis.settings as settings
import tweepy
import datetime

# In[20]:


auth = tweepy.OAuthHandler(settings.API_key, settings.API_Secret_Key)
auth.set_access_token(settings.Access_token, settings.Access_Token_Secret)
api = tweepy.API(auth)



class MyStreamListener(tweepy.StreamListener):
    def __init__(self,api=None):
        super(MyStreamListener,self).__init__()
        self.num_tweets=0 
    
    def on_status(self, status):
        if status.retweeted:
            return
        else: 
            location = status.user.location
            if location != None and "India" in location:
                with open("Realtime.txt", "a+") as text_file: 
                    with st.spinner(status.text):
                        time.sleep(5)
                    text_file.write(status.text)
                self.num_tweets+=1
                if self.num_tweets<10:
                    return True
                else:
                    return False
                    #log = open("/path/to/my/file.txt", "r")
                    #print str(log)

                
            
    
    def on_error(self, status_code):
        if status_code == 420:
            #returning False in on_data disconnects the stream
            return False


load()
datapath = 'apps/Sentiment_Analysis/models/'
fmodel = datapath + 'LSTM_RNN_Sentiment_model.pt'
fvocab = datapath + 'LSTM_RNN_Sentiment_vocab.pkl'

print(fmodel)

### Load Trained Vocabulary
def load_vocab(fpath):
  pickled_data = open(fpath, 'rb')
  vocab = pickle.load(pickled_data)
  return vocab

vocab_to_int = load_vocab(fvocab)


### SENTIMENT MODEL 
class SentimentRNN(nn.Module):
    """
    The RNN model that will be used to perform Sentiment analysis.
    """

    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):
        """
        Initialize the model by setting up the layers.
        """
        super(SentimentRNN, self).__init__()

        self.output_size = output_size
        self.n_layers = n_layers
        self.hidden_dim = hidden_dim
        
        # embedding and LSTM layers
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, 
                            dropout=drop_prob, batch_first=True)
        
        # dropout layer
        self.dropout = nn.Dropout(0.3)
        
        # linear and sigmoid layers
        self.fc = nn.Linear(hidden_dim, output_size)
        self.sig = nn.Sigmoid()
        

    def forward(self, x, hidden):
        """
        Perform a forward pass of our model on some input and hidden state.
        """
        batch_size = x.size(0)

        # embeddings and lstm_out
        x = x.long()
        embeds = self.embedding(x)
        lstm_out, hidden = self.lstm(embeds, hidden)
    
        # stack up lstm outputs
        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)
        
        # dropout and fully-connected layer
        out = self.dropout(lstm_out)
        out = self.fc(out)
        # sigmoid function
        sig_out = self.sig(out)
        
        # reshape to be batch_size first
        sig_out = sig_out.view(batch_size, -1)
        sig_out = sig_out[:, -1] # get last batch of labels
        
        # return last sigmoid output and hidden state
        return sig_out, hidden
    
    
    def init_hidden(self, batch_size):
        ''' Initializes hidden state '''
        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,
        # initialized to zero, for hidden state and cell state of LSTM
        weight = next(self.parameters()).data
        
        
        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),
                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())
        
        return hidden


def clean_text(text):
    # Convert Emoji to strings
    #text = demojize(text)

    # Remove HTML Tags
    try:
        text = html.document_fromstring(text).text_content()
    except:
        pass
          
    # Remove Hyperlinks
    text = re.sub('http\S+', ' ', text)
    
    # Remove html character codes ( like &amp)
    text = re.sub('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});', ' ', text)
    
    # Remove Hashtags
    text = re.sub("#([a-zA-Z0-9_]{1,50})", ' ', text)
    
    # Remove Mentions
    text = re.sub("@([a-zA-Z0-9_]{1,50})", ' ', text)

    # Remove non alphabets
    text = re.sub('[^a-zA-Z ]+', ' ', text)    
    
    # Remove punctuations
    text = re.sub('[^-9A-Za-z ]', ' ' , text)

    # Lowercase and split - converts to list
    text = text.lower().split()  
    
    # Remove stop and short words
    text = [word for word in text if word not in stop and len(word) > 1]
    
    # Stemming
    # text = [stemmer.stem(word) for word in text]

    # Join and Return
    return ' '.join(text )

def tokenize_text(text):

    # splitting by spaces
    test_words = text.split()

    # tokens
    test_ints = []
    test_ints.append([vocab_to_int[word] for word in test_words if vocab_to_int.get(word)])

    return test_ints

def pad_features(test_ints, seq_length):
    ''' Return features of review_ints, where each review is padded with 0's 
        or truncated to the input seq_length.
    '''
    
    # getting the correct rows x cols shape
    features = np.zeros((len(test_ints), seq_length), dtype=int)

    # for each review, I grab that review and 
    for i, row in enumerate(test_ints):
      if len(row) > 0:   # If word is not in vocabulary then there will no integer representation    
        features[i, -len(row):] = np.array(row)[:seq_length]
    
    return features


# Instantiate the model w/ hyperparams
vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding + our word tokens
output_size = 1
embedding_dim = 400
hidden_dim = 256
n_layers = 2

net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)


# Load Trained Model weights
model = net.load_state_dict(torch.load(fmodel, map_location=torch.device('cpu')))


def predict(net, test_text, sequence_length=200):
    
    net.eval()
    
    # tokenize text
    text = clean_text(test_text)
    test_ints = tokenize_text(text)
    
    # pad tokenized sequence
    seq_length=sequence_length
    features = pad_features(test_ints, seq_length)
    
    # convert to tensor to pass into your model
    feature_tensor = torch.from_numpy(features)
    
    batch_size = feature_tensor.size(0)
    
    # initialize hidden state
    h = net.init_hidden(batch_size)
  
    
    # get the output from the model
    output, h = net(feature_tensor, h)
    
    # convert output probabilities to predicted class (0 or 1)
    pred = torch.round(output.squeeze()) 
    response = int(pred.item())
    pred_value = output.item()

    return text, response, pred_value

def process_file(fdate):
  data = pd.read_csv(fdate, delimiter = "\t", quoting=csv.QUOTE_NONE, header=None)
  data.rename({0:'text'}, axis=1, inplace=True)
  a = list(data['text'])
  sentiment = []
  for input_text in a:
    if len(input_text.split(' ')) > 10:
      text, response, pred_value = predict(net, input_text, 51)
      sentiment.append(response)
  return sentiment

#@st.cache(suppress_st_warning=True)
def detect1():
    st.title('Covid Sentiment Analysis')
    option= st.sidebar.selectbox('Select Historical/Realtime', ('Realtime Tweets', 'Historical line graph'))
    TRACK_TERMS = ["corona", "covid", "COVID-19", "mask", "pandemic", "vaccine", "sanitizer"]
    if option == 'Realtime Tweets':
        terms= st.sidebar.multiselect('Select terms to track', TRACK_TERMS)
        if st.button("Get Realtime Sentiment"):
            stream_listener = MyStreamListener()
            stream = tweepy.Stream(auth=api.auth, listener=stream_listener)
            stream.filter(languages=["en"], track= terms)
            data_path = "Realtime.txt"
            result = process_file(data_path)
            c = Counter(result)
            if c.get(0) is None:
                total = c.get(1) + 0
                y = [int((c.get(1)/total)*100), 0]

            elif c.get(1) is None:
                total = c.get(0) + 0
                y = [0, int((c.get(0)/total)*100)]

            else:
                total = c.get(1) + c.get(0)
                y = [int((c.get(1)/total)*100), int((c.get(0)/total)*100)]
            x = ['Positive', 'Negative']
            
            st.write(y)
            bar_plots = [ go.Bar(x=x, y=y, name='Sentiment', marker=go.bar.Marker(color='#0343df'))]
            layout = go.Layout(
                title=go.layout.Title(text="Sentiment Analysis", x=0.5),
                yaxis_title="Percent Count", yaxis_ticksuffix = "%", yaxis_range=[0,100],
                xaxis_tickmode="array", xaxis_tickvals=list(range(27)), xaxis_ticktext=tuple(x), 
                font=dict( family="Courier New, monospace", size=24, color="Black"))
            fig = go.Figure(data=bar_plots, layout=layout)
            st.plotly_chart(fig)
            os.remove("Realtime.txt")
        

    else:
        month_no = {"January":1, "February":2, "March":3, "April":4, "May":5, "June":6, "July":7, "August":8, "September":9 , "October":10, "November":11 , "December":12}
        month = {v: k for k, v in month_no.items()}
        y=[]
        for i in range(1, 13):
            try:
                data_path = f"apps/Sentiment_Analysis/Tweet/{month[i]}2021.txt"
                result = process_file(data_path)
                print(i)
                c = Counter(result)
                total = c.get(1) + c.get(0)
                y.append(int((c.get(0)/total)*100))
            except:
                break
        x= list(range(1, i))
        #fig = go.Figure(data=[go.Scatter(x=x, y=y), title="Variation of Negative Sentiment regarding Covid-19 with months", x= 'Month', y= '%Negative Sentiment'])
        #df = pd.DataFrame(list(zip(x, y)), columns =['Month', '%Negative Sentiment'])
        fig= go.Figure()
        fig.add_trace(go.Scatter(x=x, y=y))
        fig.update_layout( title="Negative Sentiment regarding Covid-19 vs Months",
            xaxis_title="Month", yaxis_title="% Negative Sentiment", 
            font=dict( family="Courier New, monospace", size=18, color="RebeccaPurple"))
        #fig= px.scatter(df, x="Month", y="%Negative Sentiment", title="Variation of Negative Sentiment regarding Covid-19 with months")
        st.plotly_chart(fig)







    """
    date = st.date_input("Select a date", datetime.date.today(), min_value=datetime.date(2021, 8, 1))
    month = {7:"July", 8:"August", 9:"September", 10:"October"}
    data_path = f"apps/Sentiment_Analysis/Tweet/{month[date.month]}_{date.day}_2021.txt"

    result = process_file(data_path)
    c = Counter(result)
    total = c.get(1) + c.get(0)

    x = ['Positive Sentiment', 'Negative Sentiment']
    y = [int((c.get(1)/total)*100), int((c.get(0)/total)*100)]

    bar_plots = [
             go.Bar(x=x, y=y, name='Sentiment', marker=go.bar.Marker(color='#0343df'))
             ]

    layout = go.Layout(
    title=go.layout.Title(text="Sentiment Analysis", x=0.5),
    yaxis_title="Percent Count",
    yaxis_ticksuffix = "%",
    yaxis_range=[0,100],
    xaxis_tickmode="array",
    xaxis_tickvals=list(range(27)),
    xaxis_ticktext=tuple(x),
    font=dict(
            family="Courier New, monospace",
            size=24,
            color="Black"
        )
    )
    fig = go.Figure(data=bar_plots, layout=layout)
    st.plotly_chart(fig)
    """